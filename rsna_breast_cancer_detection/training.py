# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/training.ipynb.

# %% auto 0
__all__ = ['get_optimizer', 'get_scheduler', 'RSNATrainer']

# %% ../nbs/training.ipynb 1
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
from .metrics import pF1
from .data import RSNADataset
from torch.utils.data import DataLoader

from torch.cuda.amp import GradScaler, autocast
from torch import optim
from tqdm import tqdm

import pandas as pd
import numpy as np
import torch
import wandb
import os

# %% ../nbs/training.ipynb 2
def get_optimizer(optimizer_name, model, learning_rate, weight_decay):
    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias', 'BatchNorm.weight', 'BatchNorm.bias']
    optimizer_grouped_parameters = [
                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},
                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
            ]

    if optimizer_name == "adam":
        return optim.Adam(optimizer_grouped_parameters, lr=learning_rate)
    elif optimizer_name == "adamw":
        return optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)
    elif optimizer_name == "sgd":
        return optim.SGD(optimizer_grouped_parameters, lr=learning_rate)

# %% ../nbs/training.ipynb 3
def get_scheduler(cfg, optimizer, dataloader_len):
    if cfg.lr_scheduler == "cosine":
        return get_cosine_schedule_with_warmup(optimizer,
                                               num_warmup_steps=dataloader_len* cfg.scheduler_warmup_epochs,
                                               num_training_steps=dataloader_len * cfg.epochs)
    elif cfg.lr_scheduler == "onecyclelr":
        steps_per_epoch = (dataloader_len // cfg.grad_accum_iter) + dataloader_len % cfg.grad_accum_iter

        return optim.lr_scheduler.OneCycleLR(optimizer,
                                             cfg.max_learning_rate,
                                             epochs=cfg.epochs,
                                             steps_per_epoch=steps_per_epoch,
                                             pct_start=cfg.scheduler_warmup_epochs / cfg.epochs,
                                             div_factor=cfg.div_factor,
                                             final_div_factor=cfg.final_div_factor)
    else:
        return None

# %% ../nbs/training.ipynb 4
class RSNATrainer:
    def __init__(self, backbone_name, model, criterion, optimizer, 
                 config_name=None, grad_accum_steps=1, include_age=False, grad_clip_norm=10.0, 
                 lr_scheduler=None, wandb_log=False, fp16=False, device="cuda", save_path="checkpoints", 
                 validation_step=1000, first_eval_epoch=3):

        self.config_name = config_name
        self.save_path = save_path
        
        self.include_age = include_age
        self.validation_step = validation_step
        self.first_eval_epoch = first_eval_epoch
        
        self.fp16 = fp16
        self.model = model
        self.device = device
        self.wandb_log = wandb_log
        self.backbone_name = backbone_name

        self.criterion = criterion
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.grad_clip_norm = grad_clip_norm
        self.grad_accum_steps = grad_accum_steps

        self.scaler = GradScaler(enabled=fp16)
        self.pF1 = pF1()
        
        best_results_path = os.path.join(self.save_path, self.config_name, "best_results.csv")
        self.best_results_df = pd.read_csv(best_results_path, index_col=[0]) if os.path.exists(best_results_path) else \
            pd.DataFrame(columns=["fold", "pF1_mean", "pF1_max", "pF1_max_bin", "th_max", "pF1_mean_bin", "th_mean"]).set_index("fold")

    def get_postfix(self, loss, stage, get_best_threshold_and_metric=False):
        return {f"{stage} loss": f"{(loss):.3f}", 
                **(self.pF1.get_metric() if stage == "valid" else {}),
                **(self.pF1.patient_laterality_optimization() if get_best_threshold_and_metric else {})}
    
    def log_to_wandb(self, step, loss, stage, fold_i=None):
        fold = f"_fold_{fold_i}" if fold_i is not None else ""

        metrics = {
            f"{stage}/step": step,
            **{f"{stage}/{stage}_loss{fold}": loss},
            **({**{f"{stage}/{k}{fold}": v for k, v in self.pF1.get_metric().items()},
                **{f"{stage}/{k}{fold}": v for k, v in self.pF1.patient_laterality_optimization().items() if "th" not in k}} if stage == "valid" else {})
        }

        if stage == "train" and self.lr_scheduler is not None:
            metrics[f"{stage}/{stage}_lr" + fold] = self.lr_scheduler.get_last_lr()[0]

        wandb.log(metrics)
    
    def process_batch(self, batch):
        images = batch["image"].to(self.device)
        labels = batch["label"].to(self.device)
        age_id = batch["age_id"].to(self.device)
        
        if not isinstance(self.criterion, torch.nn.CrossEntropyLoss):
            labels = labels.float()
        else:
            labels = labels.squeeze()
            
        patient_lateralities = batch["patient_laterality"]

        return images, labels, patient_lateralities, (age_id if self.include_age else None)
    
    def train_one_epoch(self, epoch, train_dataloader, valid_dataloader, fold_i=None):
        self.model.train()

        total_steps = len(train_dataloader) * (epoch)
        start_step = (epoch - 1) * len(train_dataloader) + 1
        total_train_loss = 0

        with tqdm(enumerate(train_dataloader, start_step), unit="batch", total=total_steps, bar_format='{l_bar}{bar:10}{r_bar}', position=0, leave=True, initial=start_step) as progress_bar:
            progress_bar.set_description(f"Epoch {epoch}".ljust(10))
            for step, batch in progress_bar:
                internal_step = (step + 1) - start_step
                
                total_train_loss += self.train_one_step(step, batch, total_steps=total_steps)
                
                if (internal_step % self.grad_accum_steps == 0) or (step == total_steps):
                    current_step = step // self.grad_accum_steps
                    current_loss = total_train_loss / internal_step
                    
                    progress_bar.set_postfix(self.get_postfix(current_loss, "train"))
                    
                    if self.wandb_log:
                        self.log_to_wandb(current_step, current_loss, "train", fold_i)
                
                if epoch >= self.first_eval_epoch:
                    self.validate_one_epoch(step, valid_dataloader, fold_i)
                    self.model.train()
                    
        total_train_loss /= total_steps

        torch.cuda.empty_cache()
        
        return total_train_loss

    @torch.no_grad()
    def validate_one_epoch(self, global_step, dataloader, fold_i=None):
        if not (global_step % self.validation_step) == 0 or (global_step == 0):
            return

        self.model.eval()

        total_valid_loss = 0
        total_steps = len(dataloader)

        with tqdm(enumerate(dataloader, 1), unit="batch", bar_format='{l_bar}{bar:10}{r_bar}', total=total_steps, position=0, leave=True) as progress_bar:
            progress_bar.set_description(f"Validation {global_step}".ljust(15))

            for step, batch in progress_bar:
                total_valid_loss += self.validate_one_step(batch)
                progress_bar.set_postfix(self.get_postfix(total_valid_loss / step, "valid", get_best_threshold_and_metric=(step==total_steps)))
                  
        total_valid_loss /= total_steps
        
        if self.wandb_log:
            self.log_to_wandb(global_step, total_valid_loss, "valid", fold_i)
        
        self.checkpoint(fold_i)
        
        return total_valid_loss

    
    def train_one_step(self, step, batch, total_steps):
        
        images, labels, _, age_id = self.process_batch(batch)
        
        with autocast(enabled=self.fp16):
            outputs = self.model(images, age_id)
            
        batch_loss = self.criterion(outputs, labels) / self.grad_accum_steps
        self.scaler.scale(batch_loss).backward()
        
        if (step % self.grad_accum_steps == 0) or (step == total_steps):
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)

            self.scaler.step(self.optimizer)
            scale = self.scaler.get_scale()
            self.scaler.update()

            if self.lr_scheduler is not None and (scale <= self.scaler.get_scale()):
                self.lr_scheduler.step()
                
            self.optimizer.zero_grad(set_to_none=True)
            
        return batch_loss.item()
    
    @torch.no_grad()
    def validate_one_step(self, batch):
        images, labels, patient_lateralities, age_id = self.process_batch(batch)
        
        with autocast(enabled=self.fp16):
            outputs = self.model(images, age_id)
            
        batch_loss = self.criterion(outputs, labels)
        
        if isinstance(self.criterion, torch.nn.CrossEntropyLoss):
            outputs =torch.argmax(outputs, axis=1)
        else:
            outputs =torch.sigmoid(outputs)
            
        self.pF1.update(outputs.cpu(), labels.cpu(), patient_lateralities)

        return batch_loss.item()

    def train(self, epochs, train_dataloader, valid_dataloader, fold_i=None,):
        self.best_results_df.loc[len(self.best_results_df)] = 0

        if self.save_path:
            self.save_path = os.path.join(self.save_path, self.config_name, f"fold_{fold_i}")

            if not os.path.exists(self.save_path):
                os.makedirs(self.save_path)

        for epoch in range(1, epochs+1):
            self.train_one_epoch(epoch, train_dataloader, valid_dataloader, fold_i)         
        
        return self.best_results_df.loc[fold_i].to_dict()
    
    def checkpoint(self, fold_i):
        pF1s = self.pF1.get_metric()
        patient_laterality_optimization = self.pF1.patient_laterality_optimization()
        
        if pF1s["pF1_mean"] > self.best_results_df.loc[fold_i, "pF1_mean"]:
            best_pF1_mean = pF1s["pF1_mean"]

            self.best_results_df.loc[fold_i, "pF1_mean"] = best_pF1_mean
            
            torch.save(self.model.state_dict(), os.path.join(self.save_path, f"best_pF1_mean_fold_{fold_i}.pth"))
        
        if pF1s["pF1_max"] > self.best_results_df.loc[fold_i, "pF1_max"]:
            best_pF1_max = pF1s["pF1_max"]

            self.best_results_df.loc[fold_i, "pF1_max"] = best_pF1_max
            torch.save(self.model.state_dict(), os.path.join(self.save_path, f"best_pF1_max_fold_{fold_i}.pth"))
            
        if patient_laterality_optimization["pF1_mean_bin"] > self.best_results_df.loc[fold_i, "pF1_mean_bin"]:
            self.best_results_df.loc[fold_i, "pF1_mean_bin"] = patient_laterality_optimization["pF1_mean_bin"]
            self.best_results_df.loc[fold_i, "th_mean"] = patient_laterality_optimization["th_mean"]
            
            torch.save(self.model.state_dict(), os.path.join(self.save_path, f"best_pF1_mean_bin_fold_{fold_i}.pth"))
            
        if patient_laterality_optimization["pF1_max_bin"] > self.best_results_df.loc[fold_i, "pF1_max_bin"]:
            self.best_results_df.loc[fold_i, "pF1_max_bin"] = patient_laterality_optimization["pF1_max_bin"]
            self.best_results_df.loc[fold_i, "th_max"] = patient_laterality_optimization["th_max"]
            
            torch.save(self.model.state_dict(), os.path.join(self.save_path, f"best_pF1_max_bin_fold_{fold_i}.pth"))
        
        if self.save_path:
            self.best_results_df.to_csv(os.path.join(self.save_path.replace(f"fold_{fold_i}", ""), "best_results.csv"))
        
        self.pF1.reset()
