# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/models.ipynb.

# %% auto 0
__all__ = ['GeM', 'Backbone', 'RSNAModel']

# %% ../nbs/models.ipynb 1
import torch.nn.functional as F
import torch.nn as nn
import torch
import timm

# %% ../nbs/models.ipynb 2
class GeM(nn.Module):
    def __init__(self, p=3, eps=1e-6, p_trainable=False):
        super().__init__()
        if p_trainable:
            self.p = nn.Parameter(torch.ones(1) * p)
        else:
            self.p = p

        self.eps = eps

    def forward(self, x):
        return self.gem(x, p=self.p, eps=self.eps)

    def gem(self, x, p=3, eps=1e-6):
        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)

# %% ../nbs/models.ipynb 3
class Backbone(nn.Module):
    def __init__(self, name="efficientnet_b0", pretrained=True, drop_path_rate=0.):
        super().__init__()

        self.net = timm.create_model(name, pretrained=pretrained, drop_path_rate=drop_path_rate, in_chans=1)
        self.embedding_dim = self.net.get_classifier().in_features

    def forward(self, x):
        return self.net.forward_features(x)

# %% ../nbs/models.ipynb 4
class RSNAModel(nn.Module):
    def __init__(self, model_name, output_embedding_dim=512, dropout=.3, freeze_backbone_batchnorm=False, num_classes=1, drop_path_rate=0., include_age=False, multiview=False):
        super().__init__()

        self.model_name = model_name
        self.multiview = multiview
        self.include_age = include_age
        self.output_embedding_dim = output_embedding_dim

        self.backbone = Backbone(self.model_name, pretrained=True, drop_path_rate=drop_path_rate)

        if freeze_backbone_batchnorm:
            for name, child in (self.backbone.named_children()):
                if name.find('BatchNorm') != -1:
                    for param in child.parameters():
                        param.requires_grad = True

        if self.include_age:
            self.age_embedding = nn.Embedding(8, 16)

        self.global_pool = GeM()

        self.neck = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(self.backbone.embedding_dim + (16 if self.include_age else 0), self.output_embedding_dim, bias=True),
            nn.ReLU())

        self.head = nn.Linear(self.backbone.embedding_dim, num_classes)

        self.dropout1 = nn.Dropout(.1)
        self.dropout2 = nn.Dropout(.2)
        self.dropout3 = nn.Dropout(.3)
        self.dropout4 = nn.Dropout(.4)
        self.dropout5 = nn.Dropout(.5)
        
        self.neck.apply(self._init_weights)
        self.head.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, 0.0, 0.01)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

        elif isinstance(module, nn.LayerNorm) or isinstance(module, nn.BatchNorm1d):
            torch.nn.init.normal_(module.weight, 1.0, 0.01)
            torch.nn.init.zeros_(module.bias)

    def concat_with_age_embedding(self, x, age_id):
        age_embedding = self.age_embedding(age_id)
        x = torch.cat([x, age_embedding], axis=1)
        
        return x
    
    def forward_features_single(self, x, age_id=None):
        x = self.backbone(x)
        x = self.global_pool(x)
        
        x = x[:,:,0,0]
        
        if self.include_age:
            x = self.concat_with_age_embedding(x, age_id)

        return self.neck(x)
    
    def forward_features_multiview(self, x, age_id=None):
        
        if self.include_age:        
            B, V, C, H, W = x.shape
            x = x.reshape(B * V, C, H, W)

        x = self.backbone(x)
        x = self.global_pool(x)
        
        x = x[:,:,0,0]

        if self.include_age:        
            x = x.reshape(B, V, -1)
            x = torch.max(x, 1).values
            
            x = self.concat_with_age_embedding(x, age_id)
        
        return self.neck(x)
    
    def forward(self, x, age_id=None):
        if self.multiview:
            embeddings = self.forward_features_multiview(x, age_id)
        else:
            embeddings = self.forward_features_single(x, age_id)
            
        logits1 = self.head(self.dropout1(embeddings))
        logits2 = self.head(self.dropout2(embeddings))
        logits3 = self.head(self.dropout3(embeddings))
        logits4 = self.head(self.dropout4(embeddings))
        logits5 = self.head(self.dropout5(embeddings))
    
        logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5
        
        logits = self.head(embeddings)
        
        return logits
