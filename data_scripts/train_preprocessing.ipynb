{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm==0.6.5 --no-index --find-links=kaggle/input/rsna-bc-pip-requirements\n",
    "!pip install -q albumentations==1.2.1 --no-index --find-links=kaggle/input/rsna-bc-pip-requirements\n",
    "!pip install -q pylibjpeg-libjpeg==1.3.1 --no-index --find-links=/aggle/input/rsna-bc-pip-requirements\n",
    "!pip install -q pydicom==2.0.0 --no-index --find-links=kaggle/input/rsna-bc-pip-requirements\n",
    "!pip install -q python-gdcm==3.0.20 --no-index --find-links=kaggle/input/rsna-bc-pip-requirements\n",
    "!pip install -q dicomsdl==0.109.1 --no-index --find-links=kaggle/input/rsna-bc-pip-requirements\n",
    "\n",
    "!pip install -q kaggle/input/nvidia-dali-nightly-cuda110-1230dev/nvidia_dali_nightly_cuda110-1.23.0.dev20230203-7187866-py3-none-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cfg = SimpleNamespace(**{})\n",
    "cfg.IMG_SIZE = (850, 1300)\n",
    "\n",
    "cfg.DATA_DIR = \"/home/pawel/Projects/rsna-breast-cancer-detection/data/\"\n",
    "\n",
    "cfg.SAVE_DIR_TRAIN = cfg.DATA_DIR + \"DALI_VOI_train\" #\"/tmp/out/\"\n",
    "cfg.SAVE_DIR_TRAIN_ROI = cfg.DATA_DIR + \"DALI_VOI_train_roi\" #\"/tmp/out/\"\n",
    "cfg.SAVE_DIR_TRAIN_ROI_RESIZED = cfg.DATA_DIR + f\"DALI_VOI_train_roi_{cfg.IMG_SIZE[0]}x{cfg.IMG_SIZE[1]}\"\n",
    "cfg.SAVE_DIR_TRAIN_ROI_RESIZED_LB = cfg.DATA_DIR + f\"DALI_VOI_train_roi_{cfg.IMG_SIZE[0]}x{cfg.IMG_SIZE[1]}_LB\"\n",
    "\n",
    "cfg.IMAGES_DIR = cfg.DATA_DIR + \"/train_images\"\n",
    "cfg.DF_PATH = cfg.DATA_DIR + \"/train.csv\"\n",
    "cfg.JPG_DIR = \"/home/pawel/Projects/rsna-breast-cancer-detection/train_tmp/jpg/\"\n",
    "\n",
    "cfg.ROI_MODEL_PATH = '/home/pawel/Projects/rsna-breast-cancer-detection/ROI_detection/roi_detector.pt'\n",
    "cfg.YOLO_DIR = '/home/pawel/Projects/rsna-breast-cancer-detection/ROI_detection/yolov5'\n",
    "\n",
    "os.makedirs(cfg.SAVE_DIR_TRAIN, exist_ok=True)\n",
    "os.makedirs(cfg.SAVE_DIR_TRAIN_ROI, exist_ok=True)\n",
    "os.makedirs(cfg.SAVE_DIR_TRAIN_ROI_RESIZED, exist_ok=True)\n",
    "os.makedirs(cfg.SAVE_DIR_TRAIN_ROI_RESIZED_LB, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(cfg.DF_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from copy import copy\n",
    "import gc\n",
    "import shutil \n",
    "\n",
    "import glob\n",
    "from scipy.special import expit\n",
    "\n",
    "import cv2\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "import dicomsdl\n",
    "import pydicom\n",
    "import random\n",
    "\n",
    "from pydicom.filebase import DicomBytesIO\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali import pipeline_def\n",
    "from nvidia.dali.types import DALIDataType\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=3407):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to patch DALI for Int16 support\n",
    "\n",
    "\n",
    "from nvidia.dali.backend import TensorGPU, TensorListGPU\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "import nvidia.dali.ops as ops\n",
    "from nvidia.dali import types\n",
    "from nvidia.dali.plugin.base_iterator import _DaliBaseIterator\n",
    "from nvidia.dali.plugin.base_iterator import LastBatchPolicy\n",
    "import torch\n",
    "import torch.utils.dlpack as torch_dlpack\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pydicom\n",
    "\n",
    "to_torch_type = {\n",
    "    types.DALIDataType.FLOAT:   torch.float32,\n",
    "    types.DALIDataType.FLOAT64: torch.float64,\n",
    "    types.DALIDataType.FLOAT16: torch.float16,\n",
    "    types.DALIDataType.UINT8:   torch.uint8,\n",
    "    types.DALIDataType.INT8:    torch.int8,\n",
    "    types.DALIDataType.UINT16:  torch.int16,\n",
    "    types.DALIDataType.INT16:   torch.int16,\n",
    "    types.DALIDataType.INT32:   torch.int32,\n",
    "    types.DALIDataType.INT64:   torch.int64\n",
    "}\n",
    "\n",
    "\n",
    "def feed_ndarray(dali_tensor, arr, cuda_stream=None):\n",
    "    \"\"\"\n",
    "    Copy contents of DALI tensor to PyTorch's Tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    `dali_tensor` : nvidia.dali.backend.TensorCPU or nvidia.dali.backend.TensorGPU\n",
    "                    Tensor from which to copy\n",
    "    `arr` : torch.Tensor\n",
    "            Destination of the copy\n",
    "    `cuda_stream` : torch.cuda.Stream, cudaStream_t or any value that can be cast to cudaStream_t.\n",
    "                    CUDA stream to be used for the copy\n",
    "                    (if not provided, an internal user stream will be selected)\n",
    "                    In most cases, using pytorch's current stream is expected (for example,\n",
    "                    if we are copying to a tensor allocated with torch.zeros(...))\n",
    "    \"\"\"\n",
    "    dali_type = to_torch_type[dali_tensor.dtype]\n",
    "\n",
    "    assert dali_type == arr.dtype, (\"The element type of DALI Tensor/TensorList\"\n",
    "                                    \" doesn't match the element type of the target PyTorch Tensor: \"\n",
    "                                    \"{} vs {}\".format(dali_type, arr.dtype))\n",
    "    assert dali_tensor.shape() == list(arr.size()), \\\n",
    "        (\"Shapes do not match: DALI tensor has size {0}, but PyTorch Tensor has size {1}\".\n",
    "            format(dali_tensor.shape(), list(arr.size())))\n",
    "    cuda_stream = types._raw_cuda_stream(cuda_stream)\n",
    "\n",
    "    # turn raw int to a c void pointer\n",
    "    c_type_pointer = ctypes.c_void_p(arr.data_ptr())\n",
    "    if isinstance(dali_tensor, (TensorGPU, TensorListGPU)):\n",
    "        stream = None if cuda_stream is None else ctypes.c_void_p(cuda_stream)\n",
    "        dali_tensor.copy_to_external(c_type_pointer, stream, non_blocking=True)\n",
    "    else:\n",
    "        dali_tensor.copy_to_external(c_type_pointer)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len df : 54706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Len df : {len(df)}')\n",
    "df['patient_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fns\"] = df['patient_id'].astype(str) + '/' + df['image_id'].astype(str) + '.dcm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Dict, Optional, Union, List, Tuple, TYPE_CHECKING, cast, Iterable,\n",
    "    ByteString\n",
    ")\n",
    "\n",
    "def apply_voi_lut(\n",
    "    arr: \"np.ndarray\",\n",
    "    ds: \"Dataset\",\n",
    "    index: int = 0,\n",
    "    prefer_lut: bool = True\n",
    ") -> \"np.ndarray\":\n",
    "    \"\"\"Apply a VOI lookup table or windowing operation to `arr`.\n",
    "    .. versionadded:: 1.4\n",
    "    .. versionchanged:: 2.1\n",
    "        Added the `prefer_lut` keyword parameter\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : numpy.ndarray\n",
    "        The :class:`~numpy.ndarray` to apply the VOI LUT or windowing operation\n",
    "        to.\n",
    "    ds : dataset.Dataset\n",
    "        A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n",
    "        If (0028,3010) *VOI LUT Sequence* is present then returns an array\n",
    "        of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n",
    "        (0028,3002) *LUT Descriptor*. If (0028,1050) *Window Center* and\n",
    "        (0028,1051) *Window Width* are present then returns an array of\n",
    "        ``np.float64``. If neither are present then `arr` will be returned\n",
    "        unchanged.\n",
    "    index : int, optional\n",
    "        When the VOI LUT Module contains multiple alternative views, this is\n",
    "        the index of the view to return (default ``0``).\n",
    "    prefer_lut : bool\n",
    "        When the VOI LUT Module contains both *Window Width*/*Window Center*\n",
    "        and *VOI LUT Sequence*, if ``True`` (default) then apply the VOI LUT,\n",
    "        otherwise apply the windowing operation.\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array with applied VOI LUT or windowing operation.\n",
    "    Notes\n",
    "    -----\n",
    "    When the dataset requires a modality LUT or rescale operation as part of\n",
    "    the Modality LUT module then that must be applied before any windowing\n",
    "    operation.\n",
    "    See Also\n",
    "    --------\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n",
    "    References\n",
    "    ----------\n",
    "    * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n",
    "      <part03/sect_C.11.html#sect_C.11.2>`\n",
    "    * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n",
    "      <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n",
    "    * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n",
    "      <part04/sect_N.2.html#sect_N.2.1.1>`\n",
    "    \"\"\"\n",
    "    valid_voi = False\n",
    "    if ds.get('VOILUTSequence'):\n",
    "        ds.VOILUTSequence = cast(List[\"Dataset\"], ds.VOILUTSequence)\n",
    "        valid_voi = None not in [\n",
    "            ds.VOILUTSequence[0].get('LUTDescriptor', None),\n",
    "            ds.VOILUTSequence[0].get('LUTData', None)\n",
    "        ]\n",
    "    valid_windowing = None not in [\n",
    "        ds.get('WindowCenter', None),\n",
    "        ds.get('WindowWidth', None)\n",
    "    ]\n",
    "\n",
    "    if valid_voi and valid_windowing:\n",
    "        if prefer_lut:\n",
    "            return apply_voi(arr, ds, index)\n",
    "\n",
    "        return apply_windowing(arr, ds, index)\n",
    "\n",
    "    if valid_voi:\n",
    "        return apply_voi(arr, ds, index)\n",
    "\n",
    "    if valid_windowing:\n",
    "        return apply_windowing(arr, ds, index)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def apply_voi(\n",
    "    arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n",
    ") -> \"np.ndarray\":\n",
    "    \"\"\"Apply a VOI lookup table to `arr`.\n",
    "    .. versionadded:: 2.1\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : numpy.ndarray\n",
    "        The :class:`~numpy.ndarray` to apply the VOI LUT to.\n",
    "    ds : dataset.Dataset\n",
    "        A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n",
    "        If (0028,3010) *VOI LUT Sequence* is present then returns an array\n",
    "        of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n",
    "        (0028,3002) *LUT Descriptor*, otherwise `arr` will be returned\n",
    "        unchanged.\n",
    "    index : int, optional\n",
    "        When the VOI LUT Module contains multiple alternative views, this is\n",
    "        the index of the view to return (default ``0``).\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array with applied VOI LUT.\n",
    "    See Also\n",
    "    --------\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n",
    "    References\n",
    "    ----------\n",
    "    * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n",
    "      <part03/sect_C.11.html#sect_C.11.2>`\n",
    "    * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n",
    "      <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n",
    "    * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n",
    "      <part04/sect_N.2.html#sect_N.2.1.1>`\n",
    "    \"\"\"\n",
    "    if not ds.get('VOILUTSequence'):\n",
    "        return arr\n",
    "\n",
    "    if not np.issubdtype(arr.dtype, np.integer):\n",
    "        warnings.warn(\n",
    "            \"Applying a VOI LUT on a float input array may give \"\n",
    "            \"incorrect results\"\n",
    "        )\n",
    "\n",
    "    # VOI LUT Sequence contains one or more items\n",
    "    item = cast(List[\"Dataset\"], ds.VOILUTSequence)[index]\n",
    "    lut_descriptor = cast(List[int], item.LUTDescriptor)\n",
    "    nr_entries = lut_descriptor[0] or 2**16\n",
    "    first_map = lut_descriptor[1]\n",
    "\n",
    "    # PS3.3 C.8.11.3.1.5: may be 8, 10-16\n",
    "    nominal_depth = lut_descriptor[2]\n",
    "    if nominal_depth in list(range(10, 17)):\n",
    "        dtype = 'uint16'\n",
    "    elif nominal_depth == 8:\n",
    "        dtype = 'uint8'\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"'{nominal_depth}' bits per LUT entry is not supported\"\n",
    "        )\n",
    "\n",
    "    # Ambiguous VR, US or OW\n",
    "    unc_data: Iterable[int]\n",
    "    if item['LUTData'].VR == VR.OW:\n",
    "        endianness = '<' if ds.is_little_endian else '>'\n",
    "        unpack_fmt = f'{endianness}{nr_entries}H'\n",
    "        unc_data = unpack_from(unpack_fmt, cast(bytes, item.LUTData))\n",
    "    else:\n",
    "        unc_data = cast(List[int], item.LUTData)\n",
    "\n",
    "    lut_data: \"np.ndarray\" = torch.asarray(unc_data, dtype=dtype)# np.asarray(unc_data, dtype=dtype)\n",
    "\n",
    "    # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n",
    "    clipped_iv = torch.zeros(arr.shape, dtype=dtype)\n",
    "    # IVs >= `first_map` are mapped by the VOI LUT\n",
    "    # `first_map` may be negative, positive or 0\n",
    "    mapped_pixels = arr >= first_map\n",
    "    clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n",
    "    # IVs > number of entries get set to last entry\n",
    "    torch.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n",
    "\n",
    "    return lut_data[clipped_iv] #cast(\"np.ndarray\", lut_data[clipped_iv])\n",
    "\n",
    "\n",
    "def apply_windowing(\n",
    "    arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n",
    ") -> \"np.ndarray\":\n",
    "    \"\"\"Apply a windowing operation to `arr`.\n",
    "    .. versionadded:: 2.1\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : numpy.ndarray\n",
    "        The :class:`~numpy.ndarray` to apply the windowing operation to.\n",
    "    ds : dataset.Dataset\n",
    "        A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n",
    "        If (0028,1050) *Window Center* and (0028,1051) *Window Width* are\n",
    "        present then returns an array of ``np.float64``, otherwise `arr` will\n",
    "        be returned unchanged.\n",
    "    index : int, optional\n",
    "        When the VOI LUT Module contains multiple alternative views, this is\n",
    "        the index of the view to return (default ``0``).\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array with applied windowing operation.\n",
    "    Notes\n",
    "    -----\n",
    "    When the dataset requires a modality LUT or rescale operation as part of\n",
    "    the Modality LUT module then that must be applied before any windowing\n",
    "    operation.\n",
    "    See Also\n",
    "    --------\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n",
    "    :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n",
    "    References\n",
    "    ----------\n",
    "    * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n",
    "      <part03/sect_C.11.html#sect_C.11.2>`\n",
    "    * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n",
    "      <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n",
    "    * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n",
    "      <part04/sect_N.2.html#sect_N.2.1.1>`\n",
    "    \"\"\"\n",
    "    if \"WindowWidth\" not in ds and \"WindowCenter\" not in ds:\n",
    "        return arr\n",
    "\n",
    "    if ds.PhotometricInterpretation not in ['MONOCHROME1', 'MONOCHROME2']:\n",
    "        raise ValueError(\n",
    "            \"When performing a windowing operation only 'MONOCHROME1' and \"\n",
    "            \"'MONOCHROME2' are allowed for (0028,0004) Photometric \"\n",
    "            \"Interpretation\"\n",
    "        )\n",
    "\n",
    "    # May be LINEAR (default), LINEAR_EXACT, SIGMOID or not present, VM 1\n",
    "    voi_func = cast(str, getattr(ds, 'VOILUTFunction', 'LINEAR')).upper()\n",
    "    # VR DS, VM 1-n\n",
    "    elem = ds['WindowCenter']\n",
    "    center = (\n",
    "        cast(List[float], elem.value)[index] if elem.VM > 1 else elem.value\n",
    "    )\n",
    "    center = cast(float, center)\n",
    "    elem = ds['WindowWidth']\n",
    "    width = cast(List[float], elem.value)[index] if elem.VM > 1 else elem.value\n",
    "    width = cast(float, width)\n",
    "\n",
    "    # The output range depends on whether or not a modality LUT or rescale\n",
    "    #   operation has been applied\n",
    "    ds.BitsStored = cast(int, ds.BitsStored)\n",
    "    y_min: float\n",
    "    y_max: float\n",
    "    if ds.get('ModalityLUTSequence'):\n",
    "        # Unsigned - see PS3.3 C.11.1.1.1\n",
    "        y_min = 0\n",
    "        item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\n",
    "        bit_depth = cast(List[int], item.LUTDescriptor)[2]\n",
    "        y_max = 2**bit_depth - 1\n",
    "    elif ds.PixelRepresentation == 0:\n",
    "        # Unsigned\n",
    "        y_min = 0\n",
    "        y_max = 2**ds.BitsStored - 1\n",
    "    else:\n",
    "        # Signed\n",
    "        y_min = -2**(ds.BitsStored - 1)\n",
    "        y_max = 2**(ds.BitsStored - 1) - 1\n",
    "\n",
    "    slope = ds.get('RescaleSlope', None)\n",
    "    intercept = ds.get('RescaleIntercept', None)\n",
    "    if slope is not None and intercept is not None:\n",
    "        ds.RescaleSlope = cast(float, ds.RescaleSlope)\n",
    "        ds.RescaleIntercept = cast(float, ds.RescaleIntercept)\n",
    "        # Otherwise its the actual data range\n",
    "        y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\n",
    "        y_max = y_max * ds.RescaleSlope + ds.RescaleIntercept\n",
    "\n",
    "    y_range = y_max - y_min\n",
    "    arr = arr.to(torch.float64)\n",
    "\n",
    "    if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n",
    "        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n",
    "        if voi_func == 'LINEAR':\n",
    "            if width < 1:\n",
    "                raise ValueError(\n",
    "                    \"The (0028,1051) Window Width must be greater than or \"\n",
    "                    \"equal to 1 for a 'LINEAR' windowing operation\"\n",
    "                )\n",
    "            center -= 0.5\n",
    "            width -= 1\n",
    "        elif width <= 0:\n",
    "            raise ValueError(\n",
    "                \"The (0028,1051) Window Width must be greater than 0 \"\n",
    "                \"for a 'LINEAR_EXACT' windowing operation\"\n",
    "            )\n",
    "\n",
    "        below = arr <= (center - width / 2)\n",
    "        above = arr > (center + width / 2)\n",
    "        between = torch.logical_and(~below, ~above)\n",
    "\n",
    "        arr[below] = y_min\n",
    "        arr[above] = y_max\n",
    "        if between.any():\n",
    "            arr[between] = (\n",
    "                ((arr[between] - center) / width + 0.5) * y_range + y_min\n",
    "            )\n",
    "    elif voi_func == 'SIGMOID':\n",
    "        # PS3.3 C.11.2.1.3.1\n",
    "        if width <= 0:\n",
    "            raise ValueError(\n",
    "                \"The (0028,1051) Window Width must be greater than 0 \"\n",
    "                \"for a 'SIGMOID' windowing operation\"\n",
    "            )\n",
    "\n",
    "        arr = y_range / (1 + torch.exp(-4 * (arr - center) / width)) + y_min\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\"\n",
    "        )\n",
    "\n",
    "    return arr\n",
    "\n",
    "def convert_dicom_to_jpg(file, save_folder=\"\"):\n",
    "    patient = file.split('/')[-2]\n",
    "    image = file.split('/')[-1][:-4]\n",
    "    dcmfile = pydicom.dcmread(file)\n",
    "\n",
    "    if dcmfile.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.90':\n",
    "        with open(file, 'rb') as fp:\n",
    "            raw = DicomBytesIO(fp.read())\n",
    "            ds = pydicom.dcmread(raw)\n",
    "        offset = ds.PixelData.find(b\"\\x00\\x00\\x00\\x0C\")  #<---- the jpeg2000 header info we're looking for\n",
    "        hackedbitstream = bytearray()\n",
    "        hackedbitstream.extend(ds.PixelData[offset:])\n",
    "        with open(save_folder + f\"{patient}_{image}.jpg\", \"wb\") as binary_file:\n",
    "            binary_file.write(hackedbitstream)\n",
    "            \n",
    "    if dcmfile.file_meta.TransferSyntaxUID == '1.2.840.10008.1.2.4.70':\n",
    "        with open(file, 'rb') as fp:\n",
    "            raw = DicomBytesIO(fp.read())\n",
    "            ds = pydicom.dcmread(raw)\n",
    "        offset = ds.PixelData.find(b\"\\xff\\xd8\\xff\\xe0\")  #<---- the jpeg lossless header info we're looking for\n",
    "        hackedbitstream = bytearray()\n",
    "        hackedbitstream.extend(ds.PixelData[offset:])\n",
    "        with open(save_folder + f\"{patient}_{image}.jpg\", \"wb\") as binary_file:\n",
    "            binary_file.write(hackedbitstream)\n",
    "\n",
    "            \n",
    "@pipeline_def\n",
    "def jpg_decode_pipeline(jpgfiles):\n",
    "    jpegs, _ = fn.readers.file(files=jpgfiles)\n",
    "    images = fn.experimental.decoders.image(jpegs, device='mixed', output_type=types.ANY_DATA, dtype=DALIDataType.UINT16)\n",
    "    return images\n",
    "\n",
    "def parse_window_element(elem):\n",
    "    if type(elem)==list:\n",
    "        return float(elem[0])\n",
    "    if type(elem)==str:\n",
    "        return float(elem)\n",
    "    if type(elem)==float:\n",
    "        return elem\n",
    "    if type(elem)==pydicom.dataelem.DataElement:\n",
    "        try:\n",
    "            return float(elem[0])\n",
    "        except:\n",
    "            return float(elem.value)\n",
    "    return None\n",
    "\n",
    "def linear_window(data, center, width):\n",
    "    lower, upper = center - width // 2, center + width // 2\n",
    "    data = torch.clamp(data, min=lower, max=upper)\n",
    "    return data \n",
    "\n",
    "\n",
    "def process_dicom(img, dicom):\n",
    "    try:\n",
    "        invert = getattr(dicom, \"PhotometricInterpretation\", None) == \"MONOCHROME1\"\n",
    "    except:\n",
    "        invert = False\n",
    "            \n",
    "    # center = parse_window_element(dicom[\"WindowCenter\"]) \n",
    "    # width = parse_window_element(dicom[\"WindowWidth\"])\n",
    "        \n",
    "    # if (center is not None) & (width is not None):\n",
    "    #     if voi_func == \"LINEAR\":\n",
    "    #         img = linear_window(img, center, width)\n",
    "    #     elif voi_func == \"SIGMOID\":\n",
    "            \n",
    "    img = apply_voi_lut(img, dicom)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    if invert:\n",
    "        img = 1 - img\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHUNKS = len(df[\"fns\"]) // 2000 if len(df[\"fns\"]) > 2000 else 1\n",
    "CHUNKS = [(len(df[\"fns\"]) / N_CHUNKS * k, len(df[\"fns\"]) / N_CHUNKS * (k + 1)) for k in range(N_CHUNKS)]\n",
    "CHUNKS = np.array(CHUNKS).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timm 0.6.12\n",
      "import ok!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2023-1-6 Python-3.8.13 torch-1.13.1+cu116 CUDA:0 (NVIDIA RTX A4000, 16117MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 206 layers, 3087256 parameters, 0 gradients, 4.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sys\n",
    "import timm\n",
    "print('timm',timm.__version__)\n",
    "#print(timm.__file__)\n",
    "\n",
    "print('import ok!')\n",
    "\n",
    "sys.path.append(cfg.YOLO_DIR)\n",
    "from utils.general import non_max_suppression\n",
    "\n",
    "\n",
    "\n",
    "#get yolov5 and preserve plt backend\n",
    "def get_yolo():\n",
    "    b = plt.get_backend()\n",
    "    model = torch.hub.load(cfg.YOLO_DIR, 'custom', path = cfg.ROI_MODEL_PATH, source = 'local', force_reload = True)\n",
    "    matplotlib.use(b)\n",
    "    return model\n",
    "\n",
    "model = get_yolo().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _p(d):\n",
    "    if len(d) > 0 and len(d[0]) >= 1:\n",
    "        return d[0].numpy()\n",
    "    return np.array([0, 0, 1024, 1024, 1, 0])\n",
    "\n",
    "def letterbox(img, input_shape):\n",
    "    img_h, img_w = img.shape[:2]            \n",
    "    new_h, new_w = input_shape[1], input_shape[0] \n",
    "    \n",
    "    offset_h, offset_w = 0, 0                \n",
    "    if (new_w / img_w) <= (new_h / img_h):      \n",
    "        new_h = int(img_h * new_w / img_w)  \n",
    "    else:\n",
    "        new_w = int(img_w * new_h / img_h)   \n",
    "    \n",
    "    resized = cv2.resize(img, (new_w, new_h), cv2.INTER_NEAREST)\n",
    "    img = np.full((input_shape[1], input_shape[0]), 0, dtype=np.uint8)\n",
    "    img[0:new_h, 0:new_w] = resized\n",
    "\n",
    "    return img\n",
    "\n",
    "def resize_image(img, resize=None, do_letterbox=False):\n",
    "    \n",
    "    if do_letterbox:\n",
    "        return letterbox(img, resize)\n",
    "    \n",
    "    return cv2.resize(img, resize, interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:38<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:51<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 2 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:29<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 3 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:28<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 4 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:37<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 5 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:41<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 6 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2027/2027 [06:45<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 7 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:31<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 8 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:45<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 9 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:23<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 10 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:35<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 11 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:27<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 12 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:24<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 13 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2027/2027 [06:40<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 14 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:27<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 15 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:42<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 16 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:26<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 17 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:18<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 18 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:24<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 19 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:41<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 20 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2027/2027 [06:33<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 21 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:28<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 22 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:43<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 23 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:40<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 24 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:19<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 25 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2026/2026 [06:26<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 26 of 27 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2027/2027 [06:13<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DALI Raw image load complete\n"
     ]
    }
   ],
   "source": [
    "for ttt, chunk in enumerate(CHUNKS):\n",
    "    print(f'chunk {ttt} of {len(CHUNKS)} chunks')\n",
    "    os.makedirs(cfg.JPG_DIR, exist_ok=True)\n",
    "\n",
    "    _ = Parallel(n_jobs=2)(\n",
    "        delayed(convert_dicom_to_jpg)(f'{cfg.IMAGES_DIR}/{img}', save_folder=cfg.JPG_DIR)\n",
    "        for img in df[\"fns\"].tolist()[chunk[0]: chunk[1]]\n",
    "    )\n",
    "    \n",
    "    jpgfiles = glob.glob(cfg.JPG_DIR + \"*.jpg\")\n",
    "    \n",
    "\n",
    "    pipe = jpg_decode_pipeline(jpgfiles, batch_size=1, num_threads=12, device_id=0)\n",
    "    pipe.build()\n",
    "    \n",
    "    for i, f in enumerate(tqdm(jpgfiles)):\n",
    "        patient, dicom_id = f.split('/')[-1][:-4].split('_')\n",
    "        dicom = pydicom.dcmread(cfg.IMAGES_DIR + f\"/{patient}/{dicom_id}.dcm\")\n",
    "        try:\n",
    "            out = pipe.run()\n",
    "            # Dali -> Torch\n",
    "            img = out[0][0]\n",
    "            img_torch = torch.empty(img.shape(), dtype=torch.int16, device=\"cuda\")\n",
    "            feed_ndarray(img, img_torch, cuda_stream=torch.cuda.current_stream(device=0))\n",
    "            img = img_torch.float()\n",
    "            \n",
    "            #apply dicom preprocessing\n",
    "            img = process_dicom(img, dicom)\n",
    "            original_h, original_w = img.shape[:2]\n",
    "            \n",
    "            if img[:,int(-original_w * 0.10):].sum() > img[:,:int(original_w * 0.10)].sum():\n",
    "                img = torch.flip(img, dims=[1])\n",
    "            \n",
    "            image_roi = F.interpolate(img.view(1, 1, img.size(0), img.size(1)), (1024, 1024), mode=\"nearest\")[0, 0]\n",
    "            image_roi_tensor = torch.stack([image_roi, image_roi, image_roi], axis=0).unsqueeze(0)\n",
    "\n",
    "            img = (img * 255).clip(0,255).to(torch.uint8).cpu().numpy()\n",
    "            \n",
    "            out_file_train = os.path.join(cfg.SAVE_DIR_TRAIN, f\"{patient}_{dicom_id}.png\")\n",
    "            cv2.imwrite(out_file_train, img)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y = model(image_roi_tensor).cpu()\n",
    "\n",
    "            dets = non_max_suppression(y, conf_thres = 0.1, classes = [0, 1])\n",
    "\n",
    "            # keep best \n",
    "            dets = np.array([\n",
    "                _p(d) for d in dets\n",
    "            ])\n",
    "\n",
    "            h, w = 1024, 1024\n",
    "\n",
    "            cls_ = dets[:, 4].astype(np.bool_)\n",
    "            xyxy = dets[:, [0, 1, 2, 3]] / 1024\n",
    "            xyxy = np.clip(xyxy, 0, 1)\n",
    "\n",
    "            xmin, ymin, xmax, ymax = xyxy[0]\n",
    "                \n",
    "            xmin = (xmin * original_w).astype(np.uint16)\n",
    "            xmax = (xmax * original_w).astype(np.uint16)\n",
    "            ymin = (ymin * original_h).astype(np.uint16)\n",
    "            ymax = (ymax * original_h).astype(np.uint16)\n",
    "\n",
    "            img = img[ymin:ymax, xmin:xmax]\n",
    "            \n",
    "            img_resized = resize_image(img, resize=cfg.IMG_SIZE, do_letterbox=False)\n",
    "            img_lb = resize_image(img, resize=cfg.IMG_SIZE, do_letterbox=True)\n",
    "            \n",
    "            out_file_train_roi = os.path.join(cfg.SAVE_DIR_TRAIN_ROI, f\"{patient}_{dicom_id}.png\")\n",
    "            out_file_name = os.path.join(cfg.SAVE_DIR_TRAIN_ROI_RESIZED, f\"{patient}_{dicom_id}.png\")\n",
    "            out_file_name_lb = os.path.join(cfg.SAVE_DIR_TRAIN_ROI_RESIZED_LB, f\"{patient}_{dicom_id}.png\")\n",
    "            \n",
    "            cv2.imwrite(out_file_train_roi, img)\n",
    "            cv2.imwrite(out_file_name, img_resized)\n",
    "            cv2.imwrite(out_file_name_lb, img_lb)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "            pipe = jpg_decode_pipeline(jpgfiles[i+1:], batch_size=1, num_threads=2, device_id=0)\n",
    "            pipe.build()\n",
    "            continue\n",
    "\n",
    "    shutil.rmtree(cfg.JPG_DIR)\n",
    "print(f'DALI Raw image load complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc72005800b9d95ac1d07fd2ff6b224fe1312f6b526a981f0c9579b62189572"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
